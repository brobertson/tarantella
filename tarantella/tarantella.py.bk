"""Main module."""
from . import fix_kraken_hocr, dehyphenate, generate_spellcheck_file, spellcheck_hocr


def generate_image_xar(image_dir, output_dir, metadata_file, verbose, clobber):
    import glob, os, shutil, tempfile
    from os.path import basename
    from pathlib import Path
    from zipfile import ZipFile

    # test if output file exists. If it does and clobber is False, then exit without doing the following
    print("generating image xar archive")
    
    identifier = get_identifier_from_metadata_file(metadata_file)
    if verbose:
        print("identifier: ", identifier)
    temp_dir = tempfile.mkdtemp()
    output_zip_file_path = os.path.join(output_dir, identifier + "_images.xar")
    if os.path.exists(output_zip_file_path) and not (clobber == True):
        print(
            "Output xar file '",
            output_zip_file_path,
            "' exists, and you have not set --clobber, so exiting.",
        )
        return 0
    if verbose:
        print("using as tempdir: '", temp_dir, "'")

    # collect and sort all the image files in the inputdir
    types = ["*.tif", "*.png", "*.jpg", "*.tiff"]
    if verbose:
        print("output dir for all images: ", output_dir)
    all_image_files = []
    for a_type in types:
        this_type_files = glob.glob(os.path.join(image_dir, a_type))
        all_image_files += this_type_files
    all_image_files.sort()
    if verbose:
        print("Input image files:")
        print(all_image_files)

    # binarize all the images and save in a temp dir
    output_counter = 1
    for input_image_file in all_image_files:
        fileout_name = identifier + "_" + str(output_counter).zfill(4) + ".png"
        if verbose:
            print("fileout name: ", fileout_name)
        fileout_path = os.path.join(temp_dir, fileout_name)
        binarize_skimage(input_image_file, fileout_path, verbose)
        output_counter = output_counter + 1

    # save the xslt-generated metadata files to the temp dir
    with open(os.path.join(temp_dir, "expath-pkg.xml"), "w") as f:
        f.write(generate_image_expath(identifier))
    with open(os.path.join(temp_dir, "meta.xml"), "wb") as f:
        f.write(generate_image_meta(metadata_file))
    with open(os.path.join(temp_dir, "repo.xml"), "wb") as f:
        f.write(generate_image_repo(metadata_file))

    # save static metadata files to the temp dir
    static_files_dir = Path(__file__).parent / "static_for_image_xar"
    static_files = os.listdir(static_files_dir)
    for file_name in static_files:
        shutil.copy(os.path.join(static_files_dir, file_name), temp_dir)

    # generate the zip file and save to outputdir
    with ZipFile(output_zip_file_path, "w") as zipObj:
        for filename in os.listdir(temp_dir):
            filePath = os.path.join(temp_dir, filename)
            zipObj.write(filePath, basename(filePath))
    print("image archive of", output_counter, "images saved to", output_zip_file_path)


def get_identifier_from_metadata_file(metadata_file):
    import lxml.etree as ET
    tree = ET.parse(metadata_file)
    return tree.xpath("/metadata/identifier")[0].text

def generate_hocr_xar(hocr_dir, output_dir, metadata_file, dictionary_file, ocr_engine, classifier, datetime, verbose, clobber):
    import glob, os, shutil, tempfile
    import lxml.etree as etree
    import datetime as dt
    import lxml.etree as ET
    from os.path import basename
    from pathlib import Path
    from zipfile import ZipFile

    print("generating hocr xar")
    #set the datetime if it isn't given
    if (datetime == None):
        now = dt.datetime.now()
        datetime = now.strftime("%Y-%m-%d-%H-%M-%S")
        if (verbose):
            print("supplying datetime for this OCR run:", datetime)
    
    identifier = get_identifier_from_metadata_file(metadata_file)
    #get the final file name and check if we're clobbering
    output_file_name = identifier + '-' + datetime + '-' + classifier + '-texts.xar'
    output_file_path = os.path.join(output_dir, output_file_name)
    if os.path.exists(output_file_path) and not(clobber):
        print("the output file", output_file_path, "already exists, and you've set '--clobber' to false, so I'm exiting without doing anything.")
        exit(0)
    # collect and sort all the hocr files in the inputdir
    types = ["*.hocr", "*.html", "*.xhtml", "*.htm"]
    all_hocr_files = []
    for a_type in types:
        this_type_files = glob.glob(os.path.join(hocr_dir, a_type))
        all_hocr_files += this_type_files
    all_hocr_files.sort()
    if verbose:
        print("Input hocr files:")
        print(all_hocr_files)
    xhtml_temp_dir = tempfile.mkdtemp()
    output_counter = 1
    for hocr_file in all_hocr_files:
        fileout_name = identifier + "_" + str(output_counter).zfill(4) + ".html"
        #if verbose:
        #    print("fileout name: ", fileout_name)
        fileout_path = os.path.join(xhtml_temp_dir, fileout_name)
        shutil.copyfile(os.path.join(hocr_dir,hocr_file),fileout_path)
        output_counter = output_counter + 1
    xslt_to_xhtml = etree.XML(
        """\
    <xsl:stylesheet xmlns:xsl="http://www.w3.org/1999/XSL/Transform" version="1.0"
       xmlns:html='http://www.w3.org/1999/xhtml'>

       <xsl:template match="*">
        <xsl:element name="html:{local-name(.)}">
          <xsl:apply-templates select="@*|*|text()"/>
           </xsl:element>
           </xsl:template>

           <xsl:template match="@*">
             <xsl:attribute name="{name(.)}"><xsl:value-of
             select="."/></xsl:attribute>
             </xsl:template>

             </xsl:stylesheet>"""
    )
    transform_to_xhtml = etree.XSLT(xslt_to_xhtml)
    xhtml_dehyph_temp_dir = tempfile.mkdtemp()
    if (verbose):
        print("dehyphenation temp dir:", xhtml_dehyph_temp_dir)
    all_renamed_hocr_files = os.listdir(xhtml_temp_dir)
    for file_name in all_renamed_hocr_files:
        file_path = os.path.join(xhtml_temp_dir,file_name)
        with open(file_path) as file_path:
            try:
                tree = etree.parse(file_path)
                xhtml = transform_to_xhtml(tree)
                if (ocr_engine == 'kraken'):
                    fix_kraken_hocr.get_word_span_area(xhtml, verbose)
                    fix_kraken_hocr.clean_ocr_page_title(xhtml, file_name)
                    fix_kraken_hocr.share_space_spans(xhtml)
                    fix_kraken_hocr.confidence_summary(xhtml)
                dehyphenate.convert_ocrx_to_ocr(xhtml)
                dehyphenate.remove_meta_tags(xhtml)
                dehyphenate.identify(xhtml)
                dehyphenate.dehyphenate(xhtml, file_name, verbose)
                dehyphenate.add_dublin_core_tags(xhtml)
                out_path = os.path.join(xhtml_dehyph_temp_dir, file_name)
                xhtml.write(
                    out_path, pretty_print=True, xml_declaration=True, encoding="utf-8"
                )
            except Exception as e:
                print("This exception was thrown on file {}".format(file_name))
                print(e)

    #now generate a spellcheck file
    
    no_accent_dict_file_path = Path(__file__).parent / "Dictionaries/unique_no_accent_list.csv"
    #TODO: Parameterize this, so we can set the dictionary on the command line
    dictionary_file_path = Path(__file__).parent / "Dictionaries/english_greek_latin.txt"
    spellcheck_file_path = tempfile.mktemp()
    if (verbose):
        print("spellcheck file is:", spellcheck_file_path)
    generate_spellcheck_file.make_spellcheck_file(xhtml_dehyph_temp_dir, dictionary_file_path, no_accent_dict_file_path, spellcheck_file_path, verbose)
    spellchecked_xhtml_temp_dir = tempfile.mkdtemp()
    if (verbose):
        print("temp dir for collecting xar context, including spellchecked hocr: ", spellchecked_xhtml_temp_dir )
    spellcheck_hocr.spellcheck(spellcheck_file_path, xhtml_dehyph_temp_dir, spellchecked_xhtml_temp_dir, verbose)
    

    #todo delete temp files
    #make meta file for texts
    xsl_file = Path(__file__).parent / "XSLT/make_meta_texts.xsl"
    xsl_file_handle = open(xsl_file, "r")
    dom = ET.parse(open(metadata_file, 'r'))
    xslt = ET.parse(xsl_file_handle)
    plain_string_value = etree.XSLT.strparam(identifier)
    transform = ET.XSLT(xslt)
    newdom = transform(dom, identifier=etree.XSLT.strparam(identifier), classifier=etree.XSLT.strparam(classifier), rundate=etree.XSLT.strparam(datetime))
    newdom.write(os.path.join(spellchecked_xhtml_temp_dir,'meta.xml') , pretty_print=True)

    #make repo.xml for texts
    xsl_file = Path(__file__).parent / "XSLT/make_repo_texts.xsl"
    xsl_file_handle = open(xsl_file, "r")
    dom = ET.parse(metadata_file)
    xslt = ET.parse(xsl_file_handle)
    #TODO: get actual accuracy value
    transform = ET.XSLT(xslt)
    newdom = transform(dom, identifier=etree.XSLT.strparam(identifier), accuracy='0.8', rundate=etree.XSLT.strparam(datetime))
    newdom.write(os.path.join(spellchecked_xhtml_temp_dir,'repo.xml') , pretty_print=True)

    #make expath-pkg.xml for texts
    xsl_file = Path(__file__).parent / "XSLT/make_expath_texts.xsl"
    xsl_file_handle = open(xsl_file, "r")
    dom = ET.parse(metadata_file)
    xslt = ET.parse(xsl_file_handle)
    transform = ET.XSLT(xslt)
    newdom = transform(dom, identifier=etree.XSLT.strparam(identifier), rundate=etree.XSLT.strparam(datetime))
    newdom.write(os.path.join(spellchecked_xhtml_temp_dir,'expath-pkg.xml') , pretty_print=True)

    # save static metadata files to the temp dir
    static_files_dir = Path(__file__).parent / "static_for_text_xar"
    static_files = os.listdir(static_files_dir)
    for file_name in static_files:
        shutil.copy(os.path.join(static_files_dir, file_name), spellchecked_xhtml_temp_dir)

    #make accuracy report?
    #this requires the xar file, or at least images. 
    #We could re-do all this by passing in the image xar file and using its metadata for this one, which would
    #mean we don't have to keep our metadata files sitting around.
        # generate the zip file and save to outputdir

    #Make xar file output by compressing everything in 'spellchecked_xhtml_temp_dir'
    output_zip_file_path = os.path.join(output_dir, identifier + '-' + datetime + '-' + classifier + '-texts.xar')
    with ZipFile(output_zip_file_path, "w") as zipObj:
        for filename in os.listdir(spellchecked_xhtml_temp_dir):
            filePath = os.path.join(spellchecked_xhtml_temp_dir, filename)
            zipObj.write(filePath, basename(filePath))
    print("text archive from date", datetime, "saved to", output_zip_file_path)
    
    #Clean up
    if not(verbose):
        for temp_directory in [spellchecked_xhtml_temp_dir, xhtml_dehyph_temp_dir]:
            shutil.rmtree(temp_directory)
        #delete unused spellcheck file
        os.remove(spellcheck_file_path)
    #done function to generate text xar


#Helper functions
def generate_image_expath(identifier):
    return f"""<pkg:package version="1" spec="1.0">
            <xsl:attribute name="name">http://heml.mta.ca/Lace/Images/<xsl:value-of select="{identifier}"/></xsl:attribute>
      <xsl:attribute name="abbrev">
              <xsl:value-of select="{identifier}"/>
      </xsl:attribute>
      <pkg:title><xsl:value-of select="{identifier}"/>: OCR Images</pkg:title>
      <pkg:dependency package="http://heml.mta.ca/Lace/application" semver-min="0.5.7">
      </pkg:dependency>
    </pkg:package>"""


def generate_image_meta(metadata_file_handle):
    from pathlib import Path

    xsl_file = Path(__file__).parent / "XSLT/make_meta_images.xsl"
    xsl_file_handle = open(xsl_file, "r")
    return xslt(xsl_file_handle, metadata_file_handle)


def xslt(xsl_file, xml_file):
    import lxml.etree as ET

    dom = ET.parse(xml_file)
    xslt = ET.parse(xsl_file)
    transform = ET.XSLT(xslt)
    # print(transform.error_log)
    newdom = transform(dom)
    return ET.tostring(newdom, pretty_print=True)


def generate_image_repo(metadata_file):
    from pathlib import Path

    xsl_file = Path(__file__).parent / "XSLT/make_repo_images.xsl"
    xsl_file_handle = open(xsl_file, "r")
    return xslt(xsl_file_handle, metadata_file)


def binarize_skimage(filein_path, fileout_path, verbose):
    # TODO: make regional otsu
    import skimage
    from skimage import io
    from skimage.filters import threshold_otsu

    if verbose:
        print("input file to binarize: ", filein_path)
    try:
        image = io.imread(filein_path)
    except Exception as ex:
        print("raised exception trying to read: ", type(ex))
    try:
        thresh = threshold_otsu(image)
        binary_image = image > thresh
        skimage.io.imsave(fileout_path, binary_image)
    except ValueError:
        print("Error binarizing file ", filein_path, ". Passing it through unchanged.")
        skimage.io.imsave(fileout_path, image)
